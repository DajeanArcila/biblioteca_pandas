{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/PmoFE6UHxkHT8tGDlhWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DajeanArcila/biblioteca_pandas/blob/main/ejercicio_imagen_reconocer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ZQmKn16Qwfml",
        "outputId": "e98ade9c-dcf8-4a90-8c24-fb131046f982"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-93ce5390e020>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-93ce5390e020>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    data/\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "data/\n",
        "#ejemplooooo\n",
        "  person1/\n",
        "    img1.jpg\n",
        "    img2.jpg\n",
        "    ...\n",
        "  person2/\n",
        "    img1.jpg\n",
        "    img2.jpg\n",
        "    ...\n",
        "  ...\n",
        "  person10/\n",
        "    img1.jpg\n",
        "    img2.jpg\n",
        "    ...\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless dlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZd4ycJgwslz",
        "outputId": "cde9d7e6-bf11-4a93-a560-cce3dad41802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.4)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "x9Izw-ZMyDPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta a los datos y al detector de rostros\n",
        "data_path = 'data/'\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "# Crear una carpeta para las imágenes preprocesadas\n",
        "preprocessed_data_path = 'preprocessed_data/'\n",
        "os.makedirs(preprocessed_data_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "iFQk0WcWwteq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mostrar algunas imágenes de entrenamiento\n",
        "batch = next(train_data)\n",
        "images, labels = batch\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(images[i])\n",
        "    plt.title(train_data.class_indices)  # Puedes ajustar esto para mostrar las etiquetas\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pbQgETwXy_wQ",
        "outputId": "357a1945-3b83-4f25-dddf-1123aa4001e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a6e10c96af3d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Mostrar algunas imágenes de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para preprocesar imágenes\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "    if len(faces) > 0:\n",
        "        x, y, w, h = faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height()\n",
        "        face = img[y:y+h, x:x+w]\n",
        "        face = cv2.resize(face, (224, 224))\n",
        "        return face\n",
        "    return None\n",
        "\n",
        "# Preprocesar todas las imágenes y guardarlas\n",
        "for person in os.listdir(data_path):\n",
        "    person_path = os.path.join(data_path, person)\n",
        "    preprocessed_person_path = os.path.join(preprocessed_data_path, person)\n",
        "    os.makedirs(preprocessed_person_path, exist_ok=True)\n",
        "    for image_name in os.listdir(person_path):\n",
        "        image_path = os.path.join(person_path, image_name)\n",
        "        preprocessed_image = preprocess_image(image_path)\n",
        "        if preprocessed_image is not None:\n",
        "            cv2.imwrite(os.path.join(preprocessed_person_path, image_name), preprocessed_image)"
      ],
      "metadata": {
        "id": "0H13c-WNyFY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KG42GACp1JvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Cargar embeddings guardados\n",
        "with open('face_embeddings.pkl', 'rb') as f:\n",
        "    embeddings_dict = pickle.load(f)\n",
        "\n",
        "# Crear listas de embeddings y etiquetas\n",
        "known_face_encodings = []\n",
        "known_face_names = []\n",
        "\n",
        "for name, embeddings in embeddings_dict.items():\n",
        "    for embedding in embeddings:\n",
        "        known_face_encodings.append(embedding)\n",
        "        known_face_names.append(name)\n",
        "\n",
        "# Capturar video desde la cámara\n",
        "video_capture = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Capturar un solo frame de video\n",
        "    ret, frame = video_capture.read()\n",
        "\n",
        "    # Convertir la imagen de BGR (OpenCV) a RGB (face_recognition)\n",
        "    rgb_frame = frame[:, :, ::-1]\n",
        "\n",
        "    # Encontrar todas las ubicaciones de caras y sus codificaciones en el frame\n",
        "    face_locations = face_recognition.face_locations(rgb_frame)\n",
        "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
        "\n",
        "    # Recorrer cada cara encontrada en el frame\n",
        "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
        "        # Comparar la cara con nuestras caras conocidas\n",
        "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
        "        name = \"Persona no identificada\"\n",
        "\n",
        "        # Si se encontró un match, usar el primero\n",
        "        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
        "        best_match_index = np.argmin(face_distances)\n",
        "        if matches[best_match_index]:\n",
        "            name = known_face_names[best_match_index]\n",
        "\n",
        "        # Dibujar un cuadro alrededor de la cara\n",
        "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "\n",
        "        # Dibujar el nombre debajo de la cara\n",
        "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
        "        font = cv2.FONT_HERSHEY_DUPLEX\n",
        "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
        "\n",
        "    # Mostrar el frame resultante\n",
        "    cv2.imshow('Video', frame)\n",
        "\n",
        "    # Salir del loop con la tecla 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Liberar el handle de la cámara\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "6bQBBx1K0ss4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen de Pasos\n",
        "Recolectar y Preprocesar Imágenes: Asegúrate de tener imágenes bien organizadas y preprocesadas.\n",
        "Entrenar y Guardar Embeddings: Utiliza face_recognition para extraer y guardar embeddings faciales.\n",
        "Capturar Video y Reconocer Rostros: Carga los embeddings guardados y usa opencv para capturar video en tiempo real y reconocer rostros.\n",
        "Este flujo te permitirá construir un modelo de reconocimiento facial funcional y usarlo para identificar personas en tiempo real desde la cámara."
      ],
      "metadata": {
        "id": "GFNgkswX0yUj"
      }
    }
  ]
}